# huggingface_client
[![Build Status](https://github.com/shamblett/huggingface_client/actions/workflows/ci.yml/badge.svg)](https://github.com/shamblett/huggingface_client/actions/workflows/ci.yml)

A server and browser based [Hugging Face](https://huggingface.co/) REST API client for the inference and inference endpoint APIs.

The client supports standard query based inference and inference tasks for Natural Language Processing(NLP),
Audio and Vision. It also supports the inference endpoint API for creation and control of inference endpoints
and the provider endpoint.

The bindings for the inference endpoint API are generated from the [Hugging Face Inference Endpoint OpenApi specification](lib/src/openapi/spec/openapi.json)
using the [OpenAPI Generator](https://openapi-generator.tech) project.

API documentation generated by the OpenAPI Generator can be found [here](lib/src/openapi/doc).

There is no such Open API specification for the inference API.

See the [examples.md](example/example.md) document in the examples folder for usage examples.

A Hugging Face API key is needed for authentication, see [here](https://huggingface.co/docs/api-inference/quicktour) for
instructions on how to obtain this.

Using the inference API with your own inference endpoint is a simple matter of substituting
the hugging face base path with your inference endpoint URL and setting the model parameter to '' as
the inference endpoints are created on a per model(repository) basis, see [here](https://huggingface.co/docs/inference-endpoints/guides/create_endpoint)
for more details. Two examples of this are included in the example directory.

Note that the inference endpoint and provider APIs use the V2 version of the Hugging Face API. The V1 version is
deprecated by Hugging Face and should not be used, see [here](https://huggingface.co/docs/inference-endpoints/api_reference).